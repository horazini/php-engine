<a>

	Acción: Cambio de un sistema físico a través del tiempo.
	Movimiento: -
	<br>

	<h2> Fuerza </h2>

	Fuerzas: <br>Movimiento (de atracción o empuje) ejercido a un objeto.
	"causas de acción"
	<br>
	Fuerzas fundamentales: <br>Nuclear fuerte, Electromagnética, Nuclear débil, Gravitatoria
	<br><br>

	<h2>Energía</h2>

	Energía: <br>Combinación de parámetros (variables de estado, generalmente velocidad, posición, masa, etc.) que se
	transfieren de un cuerpo a otro por causa de las fuerzas, permaneciendo constantes en el sistema a través del
	tiempo.
	<br><br>
	Teorema de Noether (Conservación de la energía):<br>
	Consecuencia matemática directa de la simetría traslacional de "la cantidad conjugadas a la energía" (del tiempo).
	Las ecuaciones de evolución de los sistemas son independientes del instante de tiempo considerado, es decir, son las
	mismas en cada instante del tiempo (las leyes de la física no cambian con el tiempo).
	Eso conduce a que la "traslación" temporal sea una simetría que deja invariante las ecuaciones de evolución del
	sistema, por lo que existe una magnitud conservada, la energía.
	<br><br>
	Energías (enumeración arbitraria): En reposo (masa); Mecánica: Cinética (causada por las fuerzas) y potencial;
	Termodinámica.
	<br><br>

	Energía cinética <br>
	Los objetos en movimiento son entidades cuya posición cambia relativa a otros objetos en el mismo marco inercial.
	La energía cinética de una objeto es la energía que posee debido a su movimiento relativo. (Temporalidad)
	<br><br>

	Energía térmica o calorífica:<br>
	Energía cinética a nivel molecular.
	<br><br>

	Transferencia de energia cinética en una colisión a nivel macroscópico:
	En una colisión, los átomos de cada objeto se repelen por la fuerza electromagnética, entonces en vez de que un
	objeto traspase al otro, estos colisionan debido a la repulsión eléctrica entre los electrones orbitando cada
	nucleo.
	<br><br>

	Fuerza química:<br>
	La fuerza electromagnética es responsable de los enlaces químicos entre los átomos que crean moléculas y fuerzas
	intermoleculares.
	La fuerza electromagnética gobierna todos los procesos químicos, que surgen de las interacciones entre los
	electrones de los átomos vecinos.
	<br><br>

	Energía nuclear:
	<br><br>
	Debido a la atracción de la fuerza o interacción nuclear fuerte,
	la masa de un átomo es menor que la suma de las masas de sus constituyentes.
	<br><br>
	Defecto de masa: <br>
	Diferencia entre la masa de un átomo y la suma de las masas de sus componentes.
	<br><br>
	Energía de enlace nuclear:<br>
	Diferencia entre la energía del elemento y la energía del mismo número de protones y neutrones consideradas
	individualmente.
	Se puede medir por la cantidad mínima de energía necesaria para descomponer el núcleo en sus protones y neutrones
	separados.
	La energía emitida durante la fusión o fisión nuclear es la diferencia entre las energías de enlace del
	«combustible», es decir, el nucleido inicial (s), y los productos de fusión o fisión.
	<br><br>
	La fórmula de Einstein implica que una gran cantidad de energía puede ser liberada por la reacción de fisión nuclear
	en cadena. Es usada tanto en producción de energía atómica como en armas nucleares.
	<br>
	Reacción de fisión:
	Un nucleo de uranio-235 absorbe un neutrón, volviendose brevemento un nucleo de uranio-236 inestable.
	Se divide en elementos más ligeros que se despiden rapidamente y libera varios neutrones, así como rayos gamma.
	No entiendo si los rayos gamma son producto inherente de la fisión (separación) o de las sucesivas fusiones en
	cadena,
	como explica la teoría de arriba.
	<br>
	<br><br>
	Bosón de gauge / partícula virtual (Teoría de campo de cauge):
	<br><br>
	Partícula efímera hipotética, imposible de medir u observar, portadora de una interacción o fuerza fundamental de la
	naturaleza, al interactuar con las partículas "reales".

	<br><br>
	<h2>Irreversibilidad</h2>
	Propiedad de los procesos naturales, que significa que éstos no son reversibles en el tiempo.
	Los procesos naturales son irreversibles a menos que se añada energía extra al sistema.
	<br><br>
	En todo procesos termodinámicos macroscópicos existe una cantidad de energía que se "disipa" (no se pierde), o sea,
	que escapa al super-sistema del que es sub-sistema ese proceso, dejando al subsistema en un estado de equilibrio.
	Ejemplo: Una olla con agua hirviendo, el sistema esta fuertemente desequilibrado, pero el calor inevitablemente
	escapa al ambiente donde se encuentra la olla, ahora el ambiente esta ligeramente desequilibrado, y la olla se
	acerca a un estado de equilibrio...
	<br><br>
	Los sistemas complejos o autoorganizados, por ejemplo los seres vivos, se caracterizan por la autopoiesis, que
	permite su existencia en el tiempo. Sin embargo esta autopoiesis es sostenible ya que la vida en la tierra dispone
	de una fuente constante de energía: el sol.
	Las estrellas, cuya causa es la fuerza de la gravedad (atracción de materia a un punto) y de la nuclear (emisión de
	radiación electromagnética) tienen, según el modelo actual, un destino final inevitable, al alcanzar un punto de
	equilibrio (perdida de combustible nuclear) donde se vuelven inhertes y ya no pueden sostener a la vida.
	<br><br>
	Esto parece indicar que, aunque la energía sea siempre constante, el universo inevitablemente llegará a un estado de
	equilibrió donde las fuerzas cesarán su actividad, y como las fuerzas son la causa de las acciones, entonces, cesará
	la acción, el movimiento, y el universo será una "foto" perpetua, con la misma energía inicial que con la que
	empezó, pero en una configuración completamente inherte.


	<br><br>
	El físico alemán Rudolf Clausius, en los años 1850, fue el primero en cuantificar matemáticamente el fenómeno de la
	irreversibilidad en la naturaleza, y lo hizo a través de la introducción del concepto de entropía. En su escrito de
	1854 "Sobre la modificación del segundo teorema fundamental en la teoría mecánica del calor", Clausius afirma:
	<br><br>
	Podría ocurrir, además, que en lugar de un descenso en la transmisión de calor que acompañaría, en el único y mismo
	proceso, la transmisión en aumento, puede ocurrir otro cambio permanente, que tiene la peculiaridad de no ser
	reversible, sin que pueda tampoco ser reemplazado por un nuevo cambio permanente de una clase similar, o producir un
	descenso en la transmisión de calor.

	<br><br>
	En 1865, Clausius acuñó el término entropía (del griego ἐντροπή [entropḗ] «cambio», «giro») y



	<br><br>
	Clausius afirma la imposibilidad de que un sistema transfiera calor de un cuerpo frío a uno caliente

	<br><br>
	Apareció una paradoja al intentar reconciliar el microanálisis de un sistema con observaciones de su macroestado.

	<br><br>
	Muchos procesos son matemáticamente reversibles en su microestado cuando se analizan utilizando la mecánica
	newtoniana clásica.
	Esta paradoja claramente contamina las explicaciones microscópicas de la tendencia macroscópica hacia el equilibrio,
	como el argumento de 1860 de James Clerk Maxwell de que las colisiones moleculares implican una igualación de
	temperaturas de gases mixtos.
	<br><br>
	De 1872 a 1875, Ludwig Boltzmann reforzó la explicación estadística de esta paradoja en la forma de la fórmula de
	entropía de Boltzmann, afirmando que un aumento en el número de posibles microestados en los que podría estar un
	sistema aumentará la entropía del sistema, haciendo menos probable que el sistema volverá a un estado anterior.
	Sus fórmulas cuantificaron el análisis realizado por William Thomson, quien había argumentado que:
	<br><br>
	"Las ecuaciones de movimiento en dinámica abstracta son perfectamente reversibles; cualquier solución de estas
	ecuaciones sigue siendo válida cuando la variable de tiempo t se reemplaza por –t. Por otro lado, los procesos
	físicos son irreversibles: por ejemplo, la fricción de los sólidos, la conducción del calor y la difusión. Sin
	embargo, el principio de disipación de energía es compatible con una teoría molecular en la que cada partícula está
	sujeta a las leyes de la dinámica abstracta."
	<br><br>
	Otra explicación de los sistemas irreversibles fue presentada por el matemático francés Henri Poincaré. En 1890,
	publicó su primera explicación de la dinámica no lineal, también llamada teoría del caos. Aplicando la teoría del
	caos a la segunda ley de la termodinámica, la paradoja de la irreversibilidad se puede explicar en los errores
	asociados al escalar de microestados a macroestados y los grados de libertad utilizados al realizar observaciones
	experimentales. La sensibilidad a las condiciones iniciales relacionadas con el sistema y su entorno en los
	compuestos de microestado en una exhibición de características irreversibles dentro del ámbito físico observable.


	<br><br>
	<h2>Principios de la termodinámica</h2>
	Primer principio de la termodinámicanota (Conservación de la energía universal):
	La energia interna de un sistema es igual al calor añadido (energía de entrada) menos el trabajo realizado (energía
	de salida).
	<br><br>
	Segundo principio de la termodinámica (Irreversibilidad de los procesos naturales):
	<br><br>
	La irreversibilidad de los procesos naturales y la tendencia de los procesos naturales a conducir a la homogeneidad
	de la materia y energía (especialmente de la temperatura) implica la existencia de una cantidad llamada "entropía"
	de un sistema termodinámico.
	La cantidad de entropía del universo tiende a incrementarse en el tiempo.
	<br><br>
	Dos enunciados equivalentes:
	Enunciado de Clausius: No es posible un proceso cuyo único resultado sea la transferencia de calor de un cuerpo de
	menor temperatura a otro de mayor temperatura.
	Enunciado de Kelvin-Planck: No es posible un proceso cuyo único resultado sea la absorción de calor procedente de un
	foco y la conversión de este calor en trabajo.
	<br><br>
	xxxx
	<br><br>
	La entropía también puede considerarse como una medida física de la falta de información acerca de los detalles
	microscópicos del movimiento y la configuración del sistema, cuando solo se conocen los datos macroscópicos.
	<br><br>
	El principio afirma que, para dos estados macroscópicamente especificados de un sistema, hay una cantidad llamada
	"diferencia de entropía de la información" entre ellos.
	Esta diferencia de entropía de información define la forma en que se necesita la información microscópica adicional
	tanto para especificar uno de los estados macroscópicamente, dada la especificación macroscópica de la otra - a
	menudo un estado de referencia convenientemente elegido se puede presuponer que existe en lugar de que se indique
	expresamente.
	Una condición final de un proceso natural siempre contiene efectos microscópicamente especificables que no son
	totalmente y exactamente predecibles a partir de la especificación macroscópica de la condición inicial del proceso.
	Esta es la razón por la que la entropía incrementa en un proceso natural - el incremento nos dice cómo se necesita
	mucha información extra microscópica para distinguir el estado microscópicamente final especificado desde el
	macroestado inicial dado.
	<br><br>
	xxxxxxxxxxxxxx
	<br><br>
	la entropía, una idea nacida de la termodinámica clásica, es una entidad cuantitativa, y no cualitativa. Eso
	significa que la entropía no es algo fundamentalmente intuitivo, sino algo que se define fundamentalmente a través
	de una ecuación, a través de las matemáticas aplicadas a la física. Recuerda en tus diversas tribulaciones, que la
	entropía es lo que las ecuaciones definen que es. No existe tal cosa como una "entropía", sin una ecuación que la
	defina.
	<br><br>
	La entropía nació como una variable de estado en la termodinámica clásica. Pero el advenimiento de la mecánica
	estadística a fines del siglo XIX creó una nueva apariencia para la entropía. No pasó mucho tiempo antes de que
	Claude Shannon tomara prestada la formulación de entropía de Boltzmann-Gibbs, para usarla en su propio trabajo,
	inventando mucho de lo que ahora llamamos teoría de la información. Mi objetivo aquí es mostrar cómo funciona la
	entropía, en todos estos casos, no como un concepto borroso y mal definido, sino como una cantidad matemática y
	física claramente definida, con aplicaciones bien entendidas.

	<br><br>
	Entropía y Termodinámica Clásica
	<br><br>
	La termodinámica clásica se desarrolló durante el siglo XIX,
	En ese momento, aún no había aparecido la idea de que un gas estuviera compuesto por moléculas diminutas y que la
	temperatura representara su energía cinética promedio.
	Fue Rudolph Clausius quien primero avanzó explícitamente la idea de la entropía.
	(On Different Forms of the Fundamental Equations of the Mechanical Theory of Heat, 1865; The Mechanical Theory of
	Heat, 1867).
	<br><br>
	La definición específica, que proviene de Clausius, es S = Q/T
	Donde S es la entropía, Q es el contenido de calor del sistema y T es la temperatura del sistema.
	<br><br>
	Carnot y Clausius pensaron en el calor como una especie de fluido, una cantidad conservada que se movía de un
	sistema a otro.
	Lo que llamaron "contenido de calor", ahora lo llamaríamos más comúnmente energía térmica interna.
	<br><br>
	La temperatura del sistema es una parte explícita de esta definición clásica de entropía, y un sistema solo puede
	tener "una" temperatura (a diferencia de varias temperaturas simultáneas) si está en equilibrio termodinámico.
	Entonces, la entropía en la termodinámica clásica se define solo para sistemas que están en equilibrio
	termodinámico.

	<br><br>
	Más tarde en el siglo XIX, la teoría molecular se volvió predominante, principalmente debido a Maxwell, Thomson y
	Ludwig Boltzmann.

	<br><br>
	Entropía y Mecánica Estadística
	<br><br>
	A fines del siglo XIX, Maxwell, Ludwig Boltzmann y Josiah Willard Gibbs extendieron las ideas de la termodinámica
	clásica, a través de la nueva "teoría molecular" de los gases, al dominio que ahora llamamos mecánica estadística.
	En la termodinámica clásica, tratamos con sistemas extensivos únicos, mientras que en la mecánica estadística
	reconocemos el papel de los pequeños constituyentes del sistema. La temperatura, por ejemplo, de un sistema define
	un macroestado, mientras que la energía cinética de cada molécula en el sistema define un microestado. La variable
	de macroestado, la temperatura, se reconoce como una expresión del promedio de las variables de microestado, una
	energía cinética promedio para el sistema. Por lo tanto, si las moléculas de un gas se mueven más rápido, tienen más
	energía cinética y la temperatura aumenta naturalmente.
	<br><br>
	La ecuación 6 a continuación es la forma general de la definición de entropía en mecánica estadística, tal como la
	dedujo por primera vez Boltzmann en sus Lectures on Gas Theory (disponible como una reimpresión de Dover), pero los
	tratamientos más modernos de deducción pueden ser más fáciles de seguir, como Statistical Physics de Gregory H.
	Wannier, o The Principles of Statistical Mechanics de Richard C. Tolman (ambos también disponibles como
	reimpresiones de Dover).
	<br><br>
	S = -k * Σ[ Pᵢ * log(Pᵢ) ] Ecuación 6
	<br><br>
	In this equation, Pᵢ is the probability that particle "i" will be in a given microstate, and all of the Pi are
	evaluated for the same macrostate of the system. The symbol (an upper case Greek sigma) is a mathematical
	instruction to add up everything to the right of it. In this case, it means to add up the product of Pi times
	log(Pi) for all of the "i" particles. The "k" out in front is an arbitrary constant, which determines the units of
	measure of entropy, and in thermodynamics is Boltzmann\'s constant (1.380658×10-23 Joules/Kelvin), but its value
	could just as easily be arbitrarily set to 1 without affecting the generality of the arguments presented here. The
	negative sign is there because the probability is a number between 0 and 1, so its logarithm will always be
	negative, so the negative sign out front cancels the negative sign induced by taking the log of a number less than
	1.
	<br><br>
	Contrary to the definition seen in equation 1, neither the temperature nor the heat energy appear explicitly in this
	equation. However, the restriction that all of the microstate probabilities must be calculated for the same
	macrostate, assures that, as in the earlier case, the system must be in a state of thermal equlibrium.
	<br><br>
	Equation 6 treats the microstate probabilities individually. However, if all of the probabilities are the same, then
	we can simplify equation 6 to equation 7.
	<br><br>
	S = k * log(N) Ecuación 7
	<br><br>
	In this simplified form, the only thing we have to worry about is "N", which is the total number of microstates
	available to the system. Be careful to note that this is not the total number of particles, but rather the total
	number of microstates that the particles could occupy, with the constraint that all such microstate collections
	would show the same macrostate.


	<br><br>
	Entropy and Information Theory
	<br><br>
	The work done, primarily by Boltzmann & Gibbs, on the foundations of statistical mechanics, is of profound
	significance that can hardly be overestimated. In the hands of Clausius and his contemporarys, entropy was an
	important, but strictly thermodynamic property. Outside of physics, it simply had no meaning. But the mathematical
	foundations of statistical mechanics are applicable to any statistical system, regardless of its status as a
	thermodynamic system. So it is by the road of statistical mechanics, that we are able to talk about entropy in
	fields outside of thermodynamics, and even outside of physics per se.
	<br><br>
	Perhaps the first major excursion of entropy into new domains, comes at the hands of Claude Shannon, widely
	recognized as the father of modern communication & information theory (his classical 1948 paper A Mathematical
	Theory of Communication is on the web).
	<br><br>
	S = -k·SUM[Pilog(Pi)]
	Equation 8
	<br><br>
	If this looks familiar, it\'s not an accident. It\'s quite the same as equation 6 above, the definition of entropy
	in statistical mechanics. In A Mathematical Theory of Communication, appendix 2, Shannon proves his Theorem 2, that
	this Boltzmann entropy is the only function which satisfy\'s the requirements for a function to measure the
	uncertainty in a message (where a "message" is a string of binary bits). In this case, the constant k is recognized
	as only setting the units; it is arbitrary, and can be set equal to exactly 1 without any loss of generality (see
	the discussion in Shannon\'s paper, begining with section 6 "Choice, uncertainty and entropy"). In this case the
	probabilty Pi is the probability for the value of a given bit (usually a binary bit, but not necessarily).
	<br><br>
	In Shannon information theory, the entropy is a measure of the uncertainty over the true content of a message, but
	the task is complicated by the fact that successive bits in a string are not random, and therefore not mutually
	independent, in a real message. Also note that "information" is not a subjective quantity here, but rather an
	objective quantity, measured in bits.


	<br><br>
	Is Entropy a Measure of "Disorder"?
	<br><br>
	Let us dispense with at least one popular myth: "Entropy is disorder" is a common enough assertion, but commonality
	does not make it right. Entropy is not "disorder", although the two can be related to one another. For a good lesson
	on the traps and pitfalls of trying to assert what entropy is, see Insight into entropy by Daniel F. Styer, American
	Journal of Physics 68(12): 1090-1096 (December 2000). Styer uses liquid crystals to illustrate examples of increased
	entropy accompanying increased "order", quite impossible in the entropy is disorder worldview. And also keep in mind
	that "order" is a subjective term, and as such it is subject to the whims of interpretation. This too mitigates
	against the idea that entropy and "disorder" are always the same, a fact well illustrated by Canadian physicist Doug
	Craigen, in his online essay "Entropy, God and Evolution".
	<br><br>
	The easiest answer to the question, "What is entropy?", is to reiterate something I said in the introduction:
	Entropy is what the equations define it to be. You can interpret those equations to come up with a prosey
	explanation, but remember that the prose & the equations have to match up, because the equations give a firm,
	mathematical definition for entropy, that just won\'t go away. In classical thermodynamics, the entropy of a system
	is the ratio of heat content to temperature (equation 1), and the change in entropy represents the amount of energy
	input to the system which does not participate in mechanical work done by the system (equation 3). In statistical
	mechanics, the interpretation is more general perhaps, where the entropy becomes a function of statistical
	probability. In that case the entropy is a measure of the probability for a givem macrostate, so that a high entropy
	indicates a high probability state, and a low entropy indicates a low probability state (equation 6).
	<br><br>
	Entropy is also sometimes confused with complexity, the idea being that a more complex system must have a higher
	entropy. In fact, that is in all liklihood the opposite of reality. A system in a highly complex state is probably
	far from equilibrium and in a low entropy (improbable) state, where the equilibrium state would be simpler, less
	complex, and higher entropy.
</a>